import random
import math

#  Create K folds (list of index lists)

def make_k_folds(n, k=5, seed=42):
    """
    n: number of samples
    k: number of folds (5)
    seed: for reproducibility
    Returns: list of folds, each fold is a list of indices
    """
    indices = list(range(n))
    random.seed(seed)
    random.shuffle(indices)

    fold_size = n // k
    folds = []
    start = 0

    for i in range(k):
        # Last fold takes the remainder
        end = start + fold_size if i < k - 1 else n
        folds.append(indices[start:end])
        start = end

    return folds


# Run 5-fold cross-validation for SCRATCH Naive Bayes


def run_5fold_cv_scratch(train_df, k=5, seed=42):
    """
    train_df must have:
      - 'tokens' (list of words)
      - 'binary_label' (1=earn, 0=not-earn)

    Returns:
      - list of fold accuracies
      - mean accuracy
      - std accuracy
    """
    n = len(train_df)
    folds = make_k_folds(n, k=k, seed=seed)

    fold_accuracies = []

    # Loop over each fold as the validation fold
    for fold_id in range(k):
        valid_idx = set(folds[fold_id])
        train_idx = [i for i in range(n) if i not in valid_idx]

        # Create fold-train and fold-valid datasets
        fold_train = train_df.iloc[train_idx]
        fold_valid = train_df.iloc[list(valid_idx)]

        # ------------------------------
        # 1) Build vocabulary from fold_train only (NO leakage)
        vocab, _ = build_vocab(fold_train["tokens"].tolist(), min_freq=1)

        # ------------------------------
        # 2) Count words by class using fold_train only
        wc_earn, wc_not, total_earn, total_not = count_words_by_class(
            fold_train["tokens"].tolist(),
            fold_train["binary_label"].tolist()
        )

        # ------------------------------
        # 3) Compute word log-probabilities (Laplace smoothing)
        log_prob_earn, log_prob_not = compute_word_log_probs(
            vocab, wc_earn, wc_not, total_earn, total_not
        )

        # ------------------------------
        # 4) Compute class priors from fold_train only
        num_docs = len(fold_train)
        num_earn = sum(fold_train["binary_label"] == 1)
        num_not_earn = sum(fold_train["binary_label"] == 0)

        log_prior_earn = math.log(num_earn / num_docs)
        log_prior_not  = math.log(num_not_earn / num_docs)

        # ------------------------------
        # 5) Predict on fold_valid and compute accuracy
        y_true = fold_valid["binary_label"].tolist()
        y_pred = predict_many(
            fold_valid["tokens"].tolist(),
            vocab,
            log_prob_earn,
            log_prob_not,
            log_prior_earn,
            log_prior_not
        )

        acc = compute_accuracy(y_true, y_pred)
        fold_accuracies.append(acc)

        print(f"Fold {fold_id+1}/{k} accuracy: {acc:.4f}")

    # Mean + standard deviation (optional but nice to report)
    mean_acc = sum(fold_accuracies) / len(fold_accuracies)
    variance = sum((a - mean_acc) ** 2 for a in fold_accuracies) / len(fold_accuracies)
    std_acc = math.sqrt(variance)

    return fold_accuracies, mean_acc, std_acc


# RUN 5-FOLD CV for SCRATCH Naive Bayes

fold_accs, mean_acc, std_acc = run_5fold_cv_scratch(train_f, k=5, seed=42)

print("\nFold accuracies:", [round(a, 4) for a in fold_accs])
print("Mean accuracy:", round(mean_acc, 4))
print("Std (variation):", round(std_acc, 4))
